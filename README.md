# AI Guitar Chord Analysis ðŸŽ¸

Overlay chord labels and coaching feedback on top of your guitar videos using OpenCV + MediaPipe Hands. You supply a JSON timeline (e.g., generated by Google AI Studio); the script renders a clean overlay and points to your fret hand.

## Tech used

- **Python**: runtime
- **OpenCV**: video I/O and drawing
- **MediaPipe Hands**: fret-hand detection for on-video pointer
- **JSON**: timeline of chords + feedback

## Quick Start

### Prerequisites
- Python 3.8+
- pip

### Install
```bash
pip install -r requirements.txt
```

### Run
1) Put your files next to the script:
- your video: `your_guitar_video.mp4`
- your analysis: `guitar.json` (see schema below)

2) Edit `guitar.py` config:
```python
CONFIGS = {
    1: {
        'json_file': 'guitar.json',
        'video_file': 'your_guitar_video.mp4',
        'output_file': 'guitar_feedback.mp4',
    }
}
CURRENT_CONFIG = 1
```

3) Execute
```bash
python guitar.py
```

## Generate JSON with Google AI Studio (suggested prompt)
Upload your video and use a short, direct prompt like:

```text
This is me playing guitar chords. Identify sections where a clear chord is played.
For each section, give me:
- timestamp at the start of the section (m:ss)
- the chord label (e.g., "G maj", "D min", "A7"; short and human-readable)
- one brief improvement feedback sentence focused on fret-hand technique (e.g., fretting pressure, finger curl, avoiding string buzz, hand/wrist angle, muting, proximity to frets).
Return JSON in this exact shape:
{
  "events": [
    { "timestamp": "0:05", "chord": "G maj", "feedback": "Keep fingers closer to the frets to reduce buzz." }
  ]
}
```

## JSON Data Format

Minimum accepted structure (also accepts `events` or `chords` as the key; accepts `timestamp` or `timestamp_of_outcome` for time):

```json
{
  "events": [
    {
      "timestamp": "0:15",
      "chord": "G maj",
      "feedback": "Keep wrist relaxed; avoid muting the high E."
    }
  ]
}
```

### Field Descriptions
- `timestamp`: Start time of the chord section (format: "m:ss")
- `chord`: Display label for the chord (short text)
- `feedback`: One or two short sentences to display under the video

## Project Structure

```
ai-guitar-analysis/
â”œâ”€â”€ guitar.py              # Main overlay script
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ *.json                 # Analysis timelines (e.g., guitar.json)
â””â”€â”€ samples/               # Optional example assets
```

## Configuration Options

### Video Processing
- `process_every_n_frames`: sampling rate (~20 FPS default)
- `animation_duration`: event flash duration (1.25â€“1.5s)
- `feedback_display_duration`: overlay feedback duration per event (5s)

### Display
- Fret-hand pointer: arrow + label (left hand by default)
- Caption: centered bottom, word-wrapped
- Left-top: current chord label

## Tips
- Ensure your fret hand is visible (good lighting, not occluded)
- Keep the camera stable; hands large enough in frame helps detection
- Use a smaller-resolution video or increase `process_every_n_frames` for speed

## Troubleshooting
1. "Could not open video file"
   - Check file path/extension and that the codec is supported by OpenCV
2. "Error loading JSON file"
   - Validate JSON syntax and key names; see schema above
3. Poor hand detection
   - Improve lighting, keep hand in frame, consider higher-resolution input

## License
MIT

## Acknowledgments
- MediaPipe Hands for hand landmark detection
- OpenCV for video processing and overlays